{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e32cd2-4955-4140-8f48-9751a1a8c588",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from pathlib import Path\n",
    "import padertorch as pt\n",
    "import paderbox as pb\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from onnxruntime import InferenceSession\n",
    "from pvq_manipulation.models.vits import Vits_NT\n",
    "from pvq_manipulation.models.ffjord import FFJORD\n",
    "from IPython.display import display, Audio, clear_output\n",
    "from paderbox.transform.module_resample import resample_sox\n",
    "from pvq_manipulation.helper.vad import EnergyVAD\n",
    "from pvq_manipulation.models.hubert import HubertExtractor, SID_LARGE_LAYER\n",
    "from pvq_manipulation.helper.creapy_wrapper import process_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4df1db0-8439-4573-9dc2-5d578e8befa1",
   "metadata": {},
   "source": [
    "# load TTS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6691176-6119-4bf0-9dcf-44d657c76074",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device='cuda'\n",
    "else:\n",
    "    device='cpu'\n",
    "\n",
    "storage_dir_tts = Path(\"./Saved_models/tts_model/\")\n",
    "tts_model = Vits_NT.load_model(storage_dir_tts, checkpoint=\"checkpoint.pth\")\n",
    "tts_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c7541c-fab5-4d44-9b89-a26a34343e7c",
   "metadata": {},
   "source": [
    "# load normalizing flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a55082-c6c6-4283-96ed-217553f33bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_dir_normalizing_flow = Path(\"./Saved_models/norm_flow_creak/\")\n",
    "config_norm_flow = pb.io.load_yaml(storage_dir_normalizing_flow / \"config.yaml\")\n",
    "normalizing_flow = FFJORD.load_model(storage_dir_normalizing_flow, checkpoint=\"ckpt_best_loss.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694432d0-3fba-44a2-9fe7-4105236cc0b8",
   "metadata": {},
   "source": [
    "# load hubert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6835676-fede-4492-a0e3-14bdce78dfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hubert_model = HubertExtractor(\n",
    "    layer=SID_LARGE_LAYER,\n",
    "    model_name=\"HUBERT_LARGE\",\n",
    "    backend=\"torchaudio\",\n",
    "    device=device, \n",
    "    storage_dir= # target storage dir hubert model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78fa11b-8617-4175-902c-8af0e4491201",
   "metadata": {},
   "source": [
    "# Example Synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8afa1b-b02e-4a40-982d-36aa78f37a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_id = 1034\n",
    "example_id = \"1034_121119_000028_000001\"\n",
    "\n",
    "wav_1 = tts_model.synthesize_from_example({\n",
    "    'text' : \"It took me quite a long time to develop a voice, and now that I have it I'm not going to be silent.\", \n",
    "    'd_vector_storage_root': f\"./Saved_models/Dataset/Embeddings/{speaker_id}/{example_id}.pth\"\n",
    "})\n",
    "display(Audio(wav_1, rate=24_000, normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feeb1d62-69f2-45c1-a172-16fcfbecd0da",
   "metadata": {},
   "source": [
    "# Manipulation Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5a28b2-af2e-4b2b-aabf-9cde891c42fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_manipulation(\n",
    "    example, \n",
    "    labels,\n",
    "    flow, \n",
    "    tts_model,    d_vector,\n",
    "\n",
    "    manipulation_idx=0,\n",
    "    manipulation_fkt=1,\n",
    "):\n",
    "    labels_manipulated = labels.clone()\n",
    "    labels_manipulated[:,manipulation_idx] += manipulation_fkt\n",
    "\n",
    "    if config_norm_flow['flag_normalize_d_vectors']:\n",
    "        speaker_embedding_norm = tts_model.normalize_d_vectors(\n",
    "            d_vector,\n",
    "            Path(example['d_vector_storage_root']).parent.parent\n",
    "        )\n",
    "    elif config_norm_flow['flag_remove_mean']:\n",
    "        global_mean = pb.io.load( Path(example['d_vector_storage_root']).parent.parent / \"mean.json\")\n",
    "        global_mean = torch.tensor(global_mean, dtype=torch.float32)\n",
    "        speaker_embedding_norm = (d_vector - global_mean) \n",
    "        global_std = pb.io.load(file_path / \"std.json\")\n",
    "        global_std = torch.tensor(global_std, dtype=torch.float32)\n",
    "        speaker_embedding_norm = speaker_embedding_norm / global_std\n",
    "    else:\n",
    "        speaker_embedding_norm = d_vector\n",
    "\n",
    "    output_forward = flow.forward((speaker_embedding_norm.float(), labels))[0]\n",
    "    sampled_class_manipulated = flow.sample((output_forward, labels_manipulated))[0]\n",
    "\n",
    "    if config_norm_flow['flag_remove_mean']:\n",
    "        sampled_class_manipulated = (sampled_class_manipulated * global_std + global_mean) \n",
    "\n",
    "    wav = tts_model.synthesize_from_example({\n",
    "        'text': example['transcription'],\n",
    "        'd_vector': d_vector.detach().numpy(),\n",
    "        'd_vector_man': sampled_class_manipulated.detach().numpy(),\n",
    "        'd_vector_storage_root': example['d_vector_storage_root'],\n",
    "    })    \n",
    "    return wav\n",
    "\n",
    "def extract_speaker_embedding(example):\n",
    "    audio_data = example['loaded_audio_data']['16_000']    \n",
    "    with torch.no_grad():\n",
    "        example = tts_model.speaker_manager.prepare_example({'audio_data': {'observation': audio_data}, **example})\n",
    "        example = pt.data.utils.collate_fn([example])\n",
    "        example['features'] = torch.tensor(np.array(example['features']))\n",
    "        d_vector = tts_model.speaker_manager.forward(example)[0]\n",
    "    return d_vector\n",
    "\n",
    "pvq_labels = ['Weight', 'Resonance', 'Breathiness', 'Roughness', 'Loudness', 'Strain', 'Pitch']\n",
    "def get_creak_label(example):\n",
    "    audio_data = example['loaded_audio_data']['16_000']\n",
    "    test, y_pred, included_indices = process_file(audio_data)\n",
    "    mean_creak = np.mean(y_pred[included_indices])\n",
    "    return mean_creak * 100\n",
    "\n",
    "\n",
    "def load_speaker_labels(example, config_norm_flow, reg_stor_dir=Path('./Saved_models/pvq_extractor/')):\n",
    "    audio_data = torch.tensor(example['loaded_audio_data']['16_000'], dtype=torch.float)[None,:]\n",
    "    num_samples = torch.tensor([audio_data.shape[-1]])\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        audio_data = audio_data.cuda()\n",
    "        num_samples = num_samples.cuda()\n",
    "    providers = [\"CPUExecutionProvider\"]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        features, seq_len = hubert_model(\n",
    "            audio_data, \n",
    "            16_000, \n",
    "            sequence_lengths=num_samples,\n",
    "        )\n",
    "        \n",
    "        features = np.mean(features.squeeze(0).detach().cpu().numpy(), axis=-1)\n",
    "\n",
    "        pvqd_predictions = {}\n",
    "        for pvq in pvq_labels:\n",
    "            with open(reg_stor_dir / f\"{pvq}.onnx\", \"rb\") as fid:\n",
    "                onnx = fid.read()\n",
    "            sess = InferenceSession(onnx, providers=providers)\n",
    "            pred = sess.run(None, {\"X\": features[None]})[0].squeeze(1)\n",
    "            pvqd_predictions[pvq] = pred.tolist()[0]\n",
    "\n",
    "    pvqd_predictions['Creak_mean'] = get_creak_label(example)\n",
    "    \n",
    "    labels = []\n",
    "    for key in pvq_labels + [\"Creak_mean\"]:\n",
    "        labels.append(pvqd_predictions[key]/100)\n",
    "    return torch.tensor(labels, device=device).float()\n",
    "\n",
    "def load_audio_files(example):\n",
    "    observation_loaded, sr = pb.io.load_audio(example['audio_path']['observation'], return_sample_rate=True)\n",
    "    \n",
    "    example['loaded_audio_data'] = {}\n",
    "    observation = resample_sox(observation_loaded, in_rate=sr, out_rate=16_000)\n",
    "    \n",
    "    vad = EnergyVAD(sample_rate=16_000)\n",
    "    if observation.ndim == 1:\n",
    "        observation = observation[None, :]\n",
    "        \n",
    "    observation = vad({'audio_data': observation})['audio_data']\n",
    "    example['loaded_audio_data']['16_000'] = observation\n",
    "    \n",
    "    observation = resample_sox(observation, in_rate=sr, out_rate=24_000)\n",
    "    vad = EnergyVAD(sample_rate=24_000)\n",
    "    if observation.ndim == 1:\n",
    "        observation = observation[None, :]\n",
    "    observation = vad({'audio_data': observation})['audio_data']\n",
    "    example['loaded_audio_data']['24_000'] = observation\n",
    "    return example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008035ba-6054-4e6e-ab16-1aaaf68f584a",
   "metadata": {},
   "source": [
    "# Get example manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e921a3cd-1699-495c-b825-519fb706d89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_id = 8820\n",
    "example_id = \"8820_294120_000011_000001\"\n",
    "\n",
    "example = {\n",
    "    'audio_path': {'observation': f\"./Saved_models/Dataset/Audio_files/{example_id}.wav\"},\n",
    "    'speaker_id': speaker_id,\n",
    "    'example_id': example_id,\n",
    "    'd_vector_storage_root': f\"./Saved_models/Dataset/Embeddings/{speaker_id}/{example_id}.pth\",\n",
    "    'transcription': \"It took me quite a long time to develop a voice, and now that I have it I'm not going to be silent.\",\n",
    "}\n",
    "\n",
    "example = load_audio_files(example)\n",
    "d_vector = extract_speaker_embedding(example)\n",
    "labels = load_speaker_labels(example, config_norm_flow)\n",
    "\n",
    "wav_manipulated = get_manipulation(\n",
    "    example=example, \n",
    "    d_vector=d_vector, \n",
    "    labels=labels[None, :], \n",
    "    flow=normalizing_flow,\n",
    "    tts_model=tts_model,\n",
    "    manipulation_idx=7,\n",
    "    manipulation_fkt=3,\n",
    ")\n",
    "display(Audio(wav_manipulated, rate=24_000, normalize=True))\n",
    "display(Audio(example['audio_path']['observation'], rate=24_000, normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a04e5b-c2ab-43e5-b9df-171028100ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = pb.io.load_yaml('./Saved_models/Dataset/dataset.yaml')\n",
    "example_id_widget = widgets.Dropdown(\n",
    "    options=dataset_dict['dataset'].keys(),\n",
    "    value='1034_121119_000028_000001', \n",
    "    description='Example ID: ',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "manipulation_idx_widget = widgets.Dropdown(\n",
    "    options=[('Weight', 0), ('Resonance', 1), ('Breathiness', 2), ('Roughness', 3), ('Creak', 7)],\n",
    "    value=2, \n",
    "    description='Type:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "manipulation_fkt_widget = widgets.FloatSlider(\n",
    "    value=1.0, min=-2.0, max=5.0, step=0.1,\n",
    "    description='Strength:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "transcription_widget = widgets.Text(\n",
    "    value=\"It took me quite a long time to develop a voice, and now that I have it I'm not going to be silent.\",\n",
    "    placeholder='Type something',\n",
    "    description='String:',\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(width='900px')\n",
    ")\n",
    "\n",
    "run_button = widgets.Button(description=\"Run Manipulation\")\n",
    "\n",
    "audio_output = widgets.Output()\n",
    "\n",
    "cached_example_id = None\n",
    "cached_loaded_example = None\n",
    "cached_labels = None\n",
    "cached_d_vector = None\n",
    "\n",
    "def update_manipulation(b):\n",
    "    global cached_example_id, cached_loaded_example, cached_labels, cached_d_vector, example_database\n",
    "\n",
    "    with audio_output:\n",
    "        clear_output(wait=True)\n",
    "        display(widgets.Label(f\"Processing....\"))\n",
    "\n",
    "    example_id = example_id_widget.value.strip()\n",
    "    speaker_id = dataset_dict['dataset'][example_id]['speaker_id']\n",
    "\n",
    "    example = {\n",
    "        'audio_path': {'observation': f\"./Saved_models/Dataset/Audio_files/{example_id}.wav\"},\n",
    "        'd_vector_storage_root': f\"./Saved_models/Dataset/Embeddings/{speaker_id}/{example_id}.pth\",\n",
    "        'speaker_id': speaker_id,\n",
    "        'example_id': example_id,\n",
    "        'transcription': transcription_widget.value.strip()\n",
    "    }\n",
    "\n",
    "    if cached_example_id != example_id:\n",
    "        with audio_output:\n",
    "            clear_output(wait=True)\n",
    "            display(widgets.Label(f\"ðŸ”„ Loading new example: {example_id}\"))\n",
    "\n",
    "            cached_loaded_example = load_audio_files(example)\n",
    "            cached_d_vector = extract_speaker_embedding(cached_loaded_example)\n",
    "            cached_labels = load_speaker_labels(example, config_norm_flow)\n",
    "            cached_example_id = example_id\n",
    "            \n",
    "    wav_manipulated = get_manipulation(\n",
    "        example=cached_loaded_example, \n",
    "        d_vector=cached_d_vector, \n",
    "        labels=cached_labels[None, :], \n",
    "        flow=normalizing_flow,\n",
    "        tts_model=tts_model,\n",
    "        manipulation_idx=manipulation_idx_widget.value,\n",
    "        manipulation_fkt=manipulation_fkt_widget.value,\n",
    "    )\n",
    "    \n",
    "    with audio_output:\n",
    "        clear_output(wait=True) \n",
    "        print('Manipulated Speaker')\n",
    "        display(Audio(wav_manipulated, rate=24_000, normalize=True))\n",
    "        print('Original Speaker')\n",
    "        display(Audio(example['audio_path']['observation'], rate=24_000, normalize=True))\n",
    "\n",
    "run_button.on_click(update_manipulation)\n",
    "display(example_id_widget, manipulation_fkt_widget, transcription_widget, manipulation_idx_widget, run_button, audio_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0bb880-fc67-4933-a0ec-9e83531abcdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "voice editing",
   "language": "python",
   "name": "voice_editing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
