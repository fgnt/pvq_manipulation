{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e32cd2-4955-4140-8f48-9751a1a8c588",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from pathlib import Path\n",
    "import padertorch as pt\n",
    "import paderbox as pb\n",
    "import time\n",
    "import torch\n",
    "import torchaudio\n",
    "import ipywidgets as widgets\n",
    "from onnxruntime import InferenceSession\n",
    "from pvq_manipulation.models.vits import Vits_NT\n",
    "from pvq_manipulation.models.ffjord import FFJORD\n",
    "from IPython.display import display, Audio, clear_output\n",
    "from pvq_manipulation.models.hubert import HubertExtractor, SID_LARGE_LAYER\n",
    "from paderbox.transform.module_resample import resample_sox\n",
    "from pvq_manipulation.helper.vad import EnergyVAD\n",
    "from train_tts_nt.helper.utils import rms_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4df1db0-8439-4573-9dc2-5d578e8befa1",
   "metadata": {},
   "source": [
    "# load TTS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6691176-6119-4bf0-9dcf-44d657c76074",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_dir_tts = Path(\"./Saved_models/tts_model/\")\n",
    "tts_model = Vits_NT.load_model(storage_dir_tts, checkpoint=\"checkpoint_390000.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c7541c-fab5-4d44-9b89-a26a34343e7c",
   "metadata": {},
   "source": [
    "# load normalizing flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a55082-c6c6-4283-96ed-217553f33bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_dir_normalizing_flow = Path(\"./Saved_models/norm_flow\")\n",
    "config_norm_flow = pb.io.load_yaml(storage_dir_normalizing_flow / \"config.yaml\")\n",
    "normalizing_flow = FFJORD.load_model(storage_dir_normalizing_flow, checkpoint=\"checkpoints/ckpt_best_loss.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deebed07-b28c-49de-b30f-d80b9e1c6899",
   "metadata": {},
   "source": [
    "# load hubert features model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4627e1-bac7-4533-8cac-bbc296889855",
   "metadata": {},
   "outputs": [],
   "source": [
    "hubert_model = HubertExtractor(\n",
    "    layer=SID_LARGE_LAYER,\n",
    "    model_name=\"HUBERT_LARGE\",\n",
    "    backend=\"torchaudio\",\n",
    "    device='cpu', \n",
    "    storage_dir='/net/vol/rautenberg/storage/hubert'# target storage dir hubert model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78fa11b-8617-4175-902c-8af0e4491201",
   "metadata": {},
   "source": [
    "# Example Synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8afa1b-b02e-4a40-982d-36aa78f37a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_id = 1034\n",
    "example_id = \"1034_121119_000028_000001\"\n",
    "\n",
    "wav_1 = tts_model.synthesize_from_example({\n",
    "    'text' : \"It took me quite a long time to develop a voice, and now that I have it I'm not going to be silent.\", \n",
    "    'd_vector_storage_root': f\"./Saved_models/Dataset/Embeddings/{speaker_id}/{example_id}.pth\"\n",
    "})\n",
    "display(Audio(wav_1, rate=24_000, normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feeb1d62-69f2-45c1-a172-16fcfbecd0da",
   "metadata": {},
   "source": [
    "# Manipulation Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625368d3-dd35-4da7-a358-7bbac448806c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_manipulation(\n",
    "    example, \n",
    "    d_vector,\n",
    "    labels,\n",
    "    flow, \n",
    "    tts_model,\n",
    "    manipulation_idx=0,\n",
    "    manipulation_fkt=1,\n",
    "):\n",
    "    labels_manipulated = labels.clone()\n",
    "    labels_manipulated[:,manipulation_idx] += manipulation_fkt\n",
    "    \n",
    "    output_forward = flow.forward((d_vector.float(), labels))[0]\n",
    "    sampled_class_manipulated = flow.sample((output_forward, labels_manipulated))[0]\n",
    "\n",
    "    wav = tts_model.synthesize_from_example({\n",
    "        'text': \"It took me quite a long time to develop a voice, and now that I have it I'm not going to be silent.\",\n",
    "        'd_vector': d_vector.detach().numpy(),\n",
    "        'd_vector_man': sampled_class_manipulated.detach().numpy(),\n",
    "    })    \n",
    "    return wav\n",
    "\n",
    "def extract_speaker_embedding(example):\n",
    "    observation, sr = pb.io.load_audio(example['audio_path']['observation'], return_sample_rate=True)\n",
    "    observation = resample_sox(observation, in_rate=sr, out_rate=16_000)\n",
    "    \n",
    "    vad = EnergyVAD(sample_rate=16_000)\n",
    "    if observation.ndim == 1:\n",
    "        observation = observation[None, :]\n",
    "    \n",
    "    observation = vad({'audio_data': observation})['audio_data']\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        example = tts_model.speaker_manager.prepare_example({'audio_data': {'observation': observation}, **example})\n",
    "        example = pt.data.utils.collate_fn([example])\n",
    "        example['features'] = torch.tensor(np.array(example['features']))\n",
    "        d_vector = tts_model.speaker_manager.forward(example)[0]\n",
    "    return d_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b722e503-a8f4-4702-acce-20bcdd828846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_speaker_labels(example, config_norm_flow, reg_stor_dir=Path('./Saved_models/pvq_extractor/')):\n",
    "    audio, _ = torchaudio.load(example['audio_path']['observation'])\n",
    "    num_samples = torch.tensor([audio.shape[-1]])\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        audio = audio.cuda()\n",
    "        num_samples = num_samples.cuda()\n",
    "    providers = [\"CPUExecutionProvider\"]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        features, seq_len = hubert_model(\n",
    "            audio, \n",
    "            24_000, \n",
    "            sequence_lengths=num_samples,\n",
    "        )\n",
    "        features = np.mean(features.squeeze(0).detach().cpu().numpy(), axis=-1)\n",
    "\n",
    "        pvqd_predictions = {}\n",
    "        for pvq in ['Breathiness', 'Loudness', 'Pitch', 'Resonance', 'Roughness', 'Strain', 'Weight']:\n",
    "            with open(reg_stor_dir / f\"{pvq}.onnx\", \"rb\") as fid:\n",
    "                onnx = fid.read()\n",
    "            sess = InferenceSession(onnx, providers=providers)\n",
    "            pred = sess.run(None, {\"X\": features[None]})[0].squeeze(1)\n",
    "            pvqd_predictions[pvq] = pred.tolist()[0]\n",
    "    labels = []\n",
    "    for key in config_norm_flow['speaker_conditioning']:\n",
    "        labels.append(pvqd_predictions[key]/100)\n",
    "    return torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008035ba-6054-4e6e-ab16-1aaaf68f584a",
   "metadata": {},
   "source": [
    "# Get example manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e921a3cd-1699-495c-b825-519fb706d89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = {\n",
    "    'audio_path': {'observation': \"./Saved_models/Dataset/Audio_files/1034_121119_000028_000001.wav\"},\n",
    "    'speaker_id': 1034,\n",
    "    'example_id': \"1034_121119_000028_000001\",\n",
    "}\n",
    "\n",
    "d_vector = extract_speaker_embedding(example)\n",
    "labels = load_speaker_labels(example, config_norm_flow)\n",
    "\n",
    "wav_manipulated = get_manipulation(\n",
    "    example=example, \n",
    "    d_vector=d_vector, \n",
    "    labels=labels[None, :], \n",
    "    flow=normalizing_flow,\n",
    "    tts_model=tts_model,\n",
    "    manipulation_idx=0,\n",
    "    manipulation_fkt=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a04e5b-c2ab-43e5-b9df-171028100ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = {\n",
    "    'audio_path': {'observation': \"./Saved_models/Dataset/Audio_files/1034_121119_000028_000001.wav\"},\n",
    "    'speaker_id': 1034,\n",
    "    'example_id': \"1034_121119_000028_000001\",\n",
    "}\n",
    "\n",
    "label_options = ['Weight', 'Resonance', 'Breathiness', 'Roughness', 'Loudness', 'Strain', 'Pitch']\n",
    "\n",
    "manipulation_idx_widget = widgets.Dropdown(\n",
    "    options=[(label, i) for i, label in enumerate(label_options)],\n",
    "    value=2,  # Standardwert: Breathiness\n",
    "    description='Type:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "manipulation_fkt_widget = widgets.FloatSlider(\n",
    "    value=1.0, min=-2.0, max=2.0, step=0.1,\n",
    "    description='Strength:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "run_button = widgets.Button(description=\"Run Manipulation\")\n",
    "\n",
    "audio_output = widgets.Output()\n",
    "\n",
    "def update_manipulation(b):\n",
    "    manipulation_idx = manipulation_idx_widget.value\n",
    "    manipulation_fkt = manipulation_fkt_widget.value\n",
    "    \n",
    "    d_vector = extract_speaker_embedding(example)\n",
    "    labels = load_speaker_labels(example, config_norm_flow)\n",
    "\n",
    "    with audio_output:\n",
    "        clear_output(wait=True)\n",
    "        display(widgets.Label(\"Processing...\"))\n",
    "        \n",
    "    time.sleep(1)  \n",
    "    \n",
    "    wav_manipulated = get_manipulation(\n",
    "        example=example, \n",
    "        d_vector=d_vector, \n",
    "        labels=labels[None, :], \n",
    "        flow=normalizing_flow,\n",
    "        tts_model=tts_model,\n",
    "        manipulation_idx=manipulation_idx,\n",
    "        manipulation_fkt=manipulation_fkt,\n",
    "    )\n",
    "    \n",
    "    with audio_output:\n",
    "        clear_output(wait=True) \n",
    "        display(Audio(wav_manipulated, rate=24_000, normalize=True))\n",
    "        display(Audio(example['audio_path']['observation'], rate=24_000, normalize=True))\n",
    "\n",
    "    print(f\"Manipulated {label_options[manipulation_idx]} with strength {manipulation_fkt}\")\n",
    "\n",
    "run_button.on_click(update_manipulation)\n",
    "display(manipulation_idx_widget, manipulation_fkt_widget, run_button, audio_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "voice editing",
   "language": "python",
   "name": "voice_editing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
