{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e32cd2-4955-4140-8f48-9751a1a8c588",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "import padertorch as pt\n",
    "import paderbox as pb\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "from creapy.utils import config\n",
    "from IPython.display import display, Audio, clear_output\n",
    "from pathlib import Path\n",
    "from pvq_manipulation.models.vits import Vits_NT\n",
    "from pvq_manipulation.models.ffjord import FFJORD\n",
    "from pvq_manipulation.models.hubert import HubertExtractor, SID_LARGE_LAYER\n",
    "from pvq_manipulation.helper.manipulation_fkt import *\n",
    "\n",
    "config._CONFIG_DIR = \"./helper/creapy_config.yaml\"\n",
    "config._USER_CONFIG_DIR = \"./helper/user_config.yaml\"\n",
    "config.USER_CONFIG_DIR = \"./helper/user_config.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38506b57-f9b6-4f84-8303-97ab7d5aa1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pvq_labels = ['Weight', 'Resonance', 'Breathiness', 'Roughness', 'Loudness', 'Strain', 'Pitch', 'Creak']\n",
    "\n",
    "storage_dir_normalizing_flow = Path(\"./saved_models/flow_interspeech/\")\n",
    "\n",
    "config_norm_flow = pb.io.load_yaml(storage_dir_normalizing_flow / \"config.json\")\n",
    "normalizing_flow = FFJORD.load_model(storage_dir_normalizing_flow, checkpoint=\"model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4df1db0-8439-4573-9dc2-5d578e8befa1",
   "metadata": {},
   "source": [
    "# load TTS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6691176-6119-4bf0-9dcf-44d657c76074",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_dir_tts = Path(\"./saved_models/tts_model/\")\n",
    "tts_model = Vits_NT.load_model(storage_dir_tts, checkpoint=\"model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694432d0-3fba-44a2-9fe7-4105236cc0b8",
   "metadata": {},
   "source": [
    "# load hubert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6835676-fede-4492-a0e3-14bdce78dfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hubert_model = HubertExtractor(\n",
    "    layer=SID_LARGE_LAYER,\n",
    "    model_name=\"HUBERT_LARGE\",\n",
    "    backend=\"torchaudio\",\n",
    "    device=tts_model.device, \n",
    "    storage_dir=\"./saved_models/hubert_model/\" # target storage dir hubert model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78fa11b-8617-4175-902c-8af0e4491201",
   "metadata": {},
   "source": [
    "# Example Synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8afa1b-b02e-4a40-982d-36aa78f37a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = \"1034_121119_000028_000001\"\n",
    "\n",
    "if not Path(f\"./saved_models/audio_files/{audio_file}.pth\").is_file():\n",
    "    example = load_audio_files(f\"./saved_models/audio_files/{audio_file}.wav\")\n",
    "    speaker_embedding = extract_speaker_embedding(tts_model, example)\n",
    "    torch.save(speaker_embedding, f\"./saved_models/audio_files/{audio_file}.pth\")\n",
    "wav_1 = tts_model.synthesize_from_example({\n",
    "    'text' : \"It took me quite a long time to develop a voice, and now that I have it I'm not going to be silent.\", \n",
    "    'd_vector_storage_root': f\"./saved_models/audio_files/{audio_file}.pth\"\n",
    "})\n",
    "display(Audio(wav_1, rate=24_000, normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008035ba-6054-4e6e-ab16-1aaaf68f584a",
   "metadata": {},
   "source": [
    "# Get example manipulation\n",
    "With the manipulation the desired voice quality can be choosen and with the manipulation_intensity the degree of change. The intensity of the sampled embedding is then the estimated intensity of the input signal plus the manipulation intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e921a3cd-1699-495c-b825-519fb706d89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = {\n",
    "    'audio_file': \"1034_121119_000028_000001\",\n",
    "    'transcription': \"It took me quite a long time to develop a voice, and now that I have it I'm not going to be silent.\",\n",
    "}\n",
    "\n",
    "example = load_audio_files(example)\n",
    "labels = load_speaker_labels(example, config_norm_flow, hubert_model, pvq_labels)\n",
    "\n",
    "if not Path(f\"./saved_models/audio_files/{audio_file}.pth\").is_file():\n",
    "    speaker_embedding = extract_speaker_embedding(tts_model, example)\n",
    "    torch.save(speaker_embedding, f\"./saved_models/audio_files/{audio_file}.pth\")\n",
    "else:\n",
    "    speaker_embedding = torch.load(f\"./saved_models/audio_files/{audio_file}.pth\")\n",
    "\n",
    "wav_manipulated = get_manipulation(\n",
    "    example=example, \n",
    "    d_vector=speaker_embedding, \n",
    "    labels=labels[None, :], \n",
    "    flow=normalizing_flow,\n",
    "    tts_model=tts_model,\n",
    "    manipulation='Breathiness', # Breathiness, Creak, Roughness, Weight, Resonance\n",
    "    manipulation_intensity=1,\n",
    "    pvq_labels=pvq_labels,\n",
    ")\n",
    "display(Audio(wav_manipulated, rate=24_000, normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257c933a-ff70-410a-aa66-2d8bae2e45d5",
   "metadata": {},
   "source": [
    "# Interface with Widgets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a04e5b-c2ab-43e5-b9df-171028100ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = {\n",
    "    'audio_file': \"1034_121119_000028_000001\",\n",
    "    'transcription': \"It took me quite a long time to develop a voice, and now that I have it I'm not going to be silent.\",\n",
    "}\n",
    "\n",
    "example = load_audio_files(example)\n",
    "labels = load_speaker_labels(example, config_norm_flow, hubert_model, pvq_labels)\n",
    "if not Path(f\"./saved_models/audio_files/{audio_file}.pth\").is_file():\n",
    "    speaker_embedding = extract_speaker_embedding(tts_model, example)\n",
    "    torch.save(speaker_embedding, f\"./saved_models/audio_files/{audio_file}.pth\")\n",
    "else:\n",
    "    speaker_embedding = torch.load(f\"./saved_models/audio_files/{audio_file}.pth\")\n",
    "\n",
    "manipulation_idx_widget = widgets.Dropdown(\n",
    "    options=['Weight', 'Resonance', 'Breathiness', 'Roughness', 'Creak'],\n",
    "    value='Breathiness', \n",
    "    description='Type:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "manipulation_fkt_widget = widgets.FloatSlider(\n",
    "    value=1.0, min=-2.0, max=5.0, step=0.1,\n",
    "    description='Strength:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "transcription_widget = widgets.Text(\n",
    "    value=\"It took me quite a long time to develop a voice, and now that I have it I'm not going to be silent.\",\n",
    "    placeholder='Type something',\n",
    "    description='String:',\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(width='900px')\n",
    ")\n",
    "\n",
    "run_button = widgets.Button(description=\"Run Manipulation\")\n",
    "\n",
    "audio_output = widgets.Output()\n",
    "\n",
    "def update_manipulation(b):\n",
    "    with audio_output:\n",
    "        clear_output(wait=True)\n",
    "        print(\"üîÅ Running manipulation...\")\n",
    "        \n",
    "    wav_manipulated = get_manipulation(\n",
    "        example=example, \n",
    "        d_vector=speaker_embedding, \n",
    "        labels=labels[None, :], \n",
    "        flow=normalizing_flow,\n",
    "        tts_model=tts_model,\n",
    "        manipulation=manipulation_idx_widget.value,\n",
    "        manipulation_intensity=manipulation_fkt_widget.value,\n",
    "        pvq_labels=pvq_labels,\n",
    "    )\n",
    "    with audio_output:\n",
    "        clear_output(wait=True)\n",
    "        display(Audio(wav_manipulated, rate=24_000, normalize=True))\n",
    "\n",
    "run_button.on_click(update_manipulation)\n",
    "display(manipulation_fkt_widget, transcription_widget, manipulation_idx_widget, run_button, audio_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "-display-namepvq_test",
   "language": "python",
   "name": "-display-namepvq_test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
